name: Full Infrastructure + App Deploy + ELK Deploy

on:
  workflow_dispatch:
    inputs:
      action:
        description: "apply/destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - full-deploy
          - destroy
          - infra-only
          - scan-only
          - build-only
          - deploy-app-only
          - deploy-elk-only
          - infra+scan+app
          - infra+elk
          - infra+app+elk
          - scan+build
          - scan+deploy-app

jobs:
  terraform-apply:
    if: contains(fromJson('["infra-only", "infra+app", "infra+elk", "infra+app+elk", "infra+scan+app", "full-deploy"]'), github.event.inputs.action) #if: ${{ github.event.inputs.action == 'apply' }}
    name: "Terraform-apply"
    runs-on: ubuntu-latest
    outputs:
      bastion_ip: ${{ steps.tf_outputs.outputs.bastion_ip }}
      ec2_ip: ${{ steps.tf_outputs.outputs.ec2_ip }}
      elk_ip: ${{ steps.tf_outputs.outputs.elk_ip }}
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    defaults:
      run:
        working-directory: infra/aws/

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.0.1
          terraform_wrapper: false

      - name: Terraform Init
        run: terraform init

      - name: Terraform fmt
        run: terraform fmt -recursive

      - name: Terraform Plan
        run: terraform plan -no-color

      - name: Terraform Apply
        run: terraform apply -auto-approve

      - name: Capture Terraform Outputs
        id: tf_outputs
        run: |
          terraform output -json > outputs.json
          echo "Generated outputs.json at: $(pwd)/outputs.json"
              echo "--- outputs.json ---"
              cat outputs.json
              echo "--------------------"
          sudo apt-get install -y jq 
          echo "bastion_ip=$(jq -r '.bastion_public_ip.value' outputs.json)" >> $GITHUB_OUTPUT
          echo "ec2_ip=$(jq -r '.ec2_private_ip.value' outputs.json)" >> $GITHUB_OUTPUT
          echo "elk_ip=$(jq -r '.elk_private_ip.value' outputs.json)" >> $GITHUB_OUTPUT
          echo $bastion_ip

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: infra/aws/outputs.json

  scan:
    runs-on: ubuntu-latest
    if: contains(fromJson('["scan-only", "scan+build", "infra+scan+app", "scan+infra", "full-deploy"]'), github.event.inputs.action) #${{ github.event.inputs.action == 'apply' }}
    steps:
      # 1. Checkout Code from GitHub
      - name: Checkout Code
        uses: actions/checkout@v2

      # 2. Set up Python environment and install dependencies
      - name: Set up Python 3.x
        uses: actions/setup-python@v2
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ./app/python/requirements.txt
          pip install bandit safety

      # 3. Run Bandit scan (Python code security scanner)
      - name: Run Bandit Scan
        run: |
          bandit -r ./app/python -f json -o app/python/bandit-report.json || exit 1

      # 4. Run Safety scan (Dependency vulnerability scanner)
      - name: Run Safety Scan
        run: |
          safety check -r app/python/requirements.txt --policy-file app/python/.safety.yaml --output json > app/python/safety-report.json || exit 1

      # 5. Upload the reports for later review
      - name: Upload Bandit Report
        if: always() # Always upload, even if workflow fails
        uses: actions/upload-artifact@v4
        with:
          name: bandit-report
          path: ./app/python/bandit-report.json

      - name: Upload Safety Report
        if: always() # Always upload, even if workflow fails
        uses: actions/upload-artifact@v4
        with:
          name: safety-report
          path: ./app/python/safety-report.json

  build:
    needs: scan # Ensure deploy only happens after the scan job
    if: contains(fromJson('["build-only", "scan+build", "full-deploy"]'), github.event.inputs.action) #if: ${{ github.event.inputs.action == 'apply' && success() }}
    runs-on: ubuntu-latest
    steps:
      #1. Checkout code from the repository
      - name: Checkout code
        uses: actions/checkout@v2

      #2. Set up Docker Buildx (required for multi-platform builds)
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      #3. Set up Docker
      - name: Set up Docker
        uses: docker/setup-qemu-action@v2

      #4. Build the Docker image
      - name: Build Docker image
        run: |
          docker build -t michaelkedey/jomacsdevsecops:latest -f app/containerization/Dockerfile app/python/

      #5. Install Trivy
      - name: Install Trivy
        run: |
          curl -sfL https://github.com/aquasecurity/trivy/releases/download/v0.40.0/trivy_0.40.0_Linux-64bit.deb -o trivy.deb
          sudo dpkg -i trivy.deb

      #6. Scan the Docker image for vulnerabilities using Trivy
      - name: Scan Docker image for vulnerabilities
        id: trivy_scan
        run: |
          mkdir -p trivy-reports
          trivy image --no-progress --severity HIGH,CRITICAL --format json --output trivy-reports/scan-report.json michaelkedey/jomacsdevsecops:latest

      #7. Upload the Trivy scan report as an artifact (so you can download it later)
      - name: Upload Trivy scan report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-scan-report
          path: trivy-reports/scan-report.json

      #8. If no vulnerabilities with high severity are found, push to Docker Hub
      - name: Push Docker image to Docker Hub
        if: ${{ success() }}
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
        run: |
          echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin
          docker push michaelkedey/jomacsdevsecops:latest

  deploy-app:
    needs: [terraform-apply, scan] # Ensure deploy only happens after the scan job
    if: contains(fromJson('["deploy-app-only", "infra+app", "infra+app+elk", "infra+scan+app", "full-deploy"]'), github.event.inputs.action) #if: ${{ github.event.inputs.action == 'apply' && success() }}  # Only run if the scan job succeeds
    runs-on: ubuntu-latest
    steps:
      # 1. Checkout Code from GitHub
      - name: Checkout Code
        uses: actions/checkout@v2

      # 2. Set up SSH key and establish SSH Tunnel
      - name: Set up SSH tunnel and deploy code
        env:
          PRIVATE_KEY: ${{ secrets.EC2_SSH_KEY }}
          BASTION: ${{ needs.terraform-apply.outputs.bastion_ip }}
          HOST: ${{ needs.terraform-apply.outputs.ec2_ip }}
          USER: ${{ secrets.EC2_USER }}
          PORT: ${{ secrets.SSH_PORT }}
          TUNNEL: ${{ secrets.SSH_TUNNEL_PORT }}
        run: |
          echo "waiting for instances to be ready"
          sleep 60

          echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

          # Establish the SSH tunnel from GitHub Actions to the target EC2 instance
          # This command forwards the local port ${TUNNEL} to the target EC2 instance's SSH  
          ssh -i private_key.pem -o StrictHostKeyChecking=no -tt -L ${TUNNEL}:${HOST}:${PORT} -p ${PORT} ${USER}@${BASTION} -N -f
      
      # 3. copy code and deploy
      - name: Deploy app and start Gunicorn
        env:
          USER: ${{ secrets.EC2_USER }}
          TUNNEL: ${{ secrets.SSH_TUNNEL_PORT }}
          HOST: ${{ needs.terraform-apply.outputs.ec2_ip }}
          LOGSTASH_IP: ${{ needs.terraform-apply.outputs.elk_ip }}
          LOGSTASH_PORT: ${{ secrets.LOGSTASH_PORT }}
          APP_PORT: ${{ secrets.APP_PORT }}
        run: |
          rsync -avz -e "ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no" ./app/python/ ${USER}@localhost:/home/${USER}/my-app/

          # Export environment variables for the SSH session
          export LOGSTASH_IP="${LOGSTASH_IP}"
          export USER="${USER}"
          export LOGSTASH_PORT="${LOGSTASH_PORT}"
          export APP_PORT="${APP_PORT}"

          # SSH into the EC2 instance and install dependencies and start Gunicorn
          ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no ${USER}@localhost << EOF
            # Install dependencies
            cd /home/${USER}/my-app/
            sudo apt-get update -y -y && sudo apt-get upgrade -y
            sudo apt-get install python3-pip -y
            sudo apt-get install gunicorn -y
            sudo apt-get install python3-flask -y
            pip install -r requirements.txt
            echo "Creating necessary log directories..."
            sudo mkdir -p logs
            sudo chown $USER:$USER logs
            echo "starting Gunicorn"
            nohup gunicorn -w 4 -b 127.0.0.1:${APP_PORT} app:app \
            > /home/$USER/my-app/logs/access.log 2> /home/$USER/my-app/logs/error.log &

            echo "Nginx Permissions"
            sudo chmod +x /home/${USER}
            sudo chmod +x /home/${USER}/my-app
            sudo chmod +x /home/${USER}/my-app/static

            echo "installing nginx for proxy"
            if ! systemctl list-units --type=service --all | grep -q nginx.service; then
              sudo apt-get update -y && sudo apt-get upgrade -y
              sudo apt-get install nginx -y
              sudo systemctl enable nginx 
              sudo systemctl start nginx
            else 
              echo "Nginx is already installed"
              sudo systemctl restart nginx
            fi

            echo ">>> Creating Nginx config for APP"
            sudo rm /etc/nginx/sites-available/default
            sudo rm /etc/nginx/sites-enabled/default
            sudo tee /etc/nginx/sites-available/default > /dev/null <<NGINX
            server {
              listen 80 default_server;
              server_name _;

              # Static files
              location /app/static/ {
                  alias /home/ubuntu/my-app/static/;
                  expires 30d;
                  access_log off;
              }

              # Reverse proxy for Flask app
              location /app/ {
                  proxy_pass http://127.0.0.1:${APP_PORT}/;

                  proxy_set_header Host \$host;
                  proxy_set_header X-Real-IP \$remote_addr;
                  proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
                  proxy_set_header X-Forwarded-Proto \$scheme;
              }
            }
      
          NGINX

            echo "Enabling Nginx site"
            sudo ln -sf /etc/nginx/sites-available/default /etc/nginx/sites-enabled/
            sudo nginx -t
            sudo systemctl restart nginx
            sudo systemctl status nginx

            echo "install filebeat for logs"
            if ! systemctl list-units --type=service --all | grep -q filebeat.service; then
              wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
              sudo apt-get install apt-transport-https
              # Add Elastic 8.x APT repo
              echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
              sudo apt-get update && sudo apt-get install filebeat
              sudo systemctl enable filebeat
              sudo systemctl start filebeat
            else 
              echo "Filebeat is already installed"
              sudo systemctl restart filebeat
            fi

            echo "Configuring Filebeat to read logs..."
            sudo tee /etc/filebeat/filebeat.yml > /dev/null << FILEBEAT
            filebeat.inputs:
              - type: log
                enabled: true
                paths:
                  - /home/${USER}/my-app/logs/access.log
                  - /home/${USER}/my-app/logs/error.log
                fields:
                  app: my-flask-app
                fields_under_root: true

            output.logstash:
              hosts: ["http://${LOGSTASH_IP}:${LOGSTASH_PORT}"] 
          FILEBEAT

          echo "restarting filebeat"
          sudo systemctl restart filebeat
          sudo systemctl status filebeat

          EOF

        # 4. Clean-up tunnel
      - name: Cleanup SSH Tunnel
        if: always() # Runs even if previous steps fail
        run: |
          # Kill tunnel process using port ${TUNNEL}
          pkill -f "ssh.*${TUNNEL}:.*${HOST}"
          echo "SSH tunnel terminated"

  deploy-elk:
    needs: terraform-apply # Ensure deploy only happens after the scan job
    if: contains(fromJson('["deploy-elk-only", "infra+elk", "infra+app+elk", "full-deploy"]'), github.event.inputs.action) #if: ${{ github.event.inputs.action == 'apply' && success() }}  # Only run if the app deploy job succeeds
    runs-on: ubuntu-latest
    steps:
      # 1. Checkout Code from GitHub
      - name: Checkout Code
        uses: actions/checkout@v2
      # 2. Set up SSH key and establish SSH Tunnel
      - name: Set up SSH tunnel and deploy code
        env:
          PRIVATE_KEY: ${{ secrets.EC2_SSH_KEY }}
          BASTION: ${{ needs.terraform-apply.outputs.bastion_ip }}
          ELK: ${{ needs.terraform-apply.outputs.elk_ip }}
          USER: ${{ secrets.EC2_USER }}
          PORT: ${{ secrets.SSH_PORT }}
          TUNNEL: ${{ secrets.ELK_TUNNEL_PORT }}
        run: |
          echo "waiting for instances to be ready"
          sleep 60

          echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

          # Establish the SSH tunnel from GitHub Actions to the target EC2 instance
          # This command forwards the local port ${TUNNEL} to the target EC2 instance's SSH  
          ssh -i private_key.pem -o StrictHostKeyChecking=no -tt -L ${TUNNEL}:${ELK}:${PORT} -p ${PORT} ${USER}@${BASTION} -N -f
      # 3. deploy elk
      - name: Deploy and start elk stack
        env:
          USER: ${{ secrets.EC2_USER }}
          TUNNEL: ${{ secrets.ELK_TUNNEL_PORT }}
          PASSWORD: ${{ secrets.KIBANA_PASSWORD }}
          ELK: ${{ needs.terraform-apply.outputs.elk_ip }}
          LOGSTASH_PORT: ${{ secrets.LOGSTASH_PORT }}
          ELASTICSEARCH_PORT: ${{ secrets.ELASTICSEARCH_PORT }}
        run: |
          export LOGSTASH_PORT=${{ secrets.LOGSTASH_PORT }}
          export ELASTICSEARCH_PORT=${{ secrets.ELASTICSEARCH_PORT }}

          # SSH into the EC2 instance and install elk and start it
          ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no ${USER}@localhost << EOF          
            # Update package index
            sudo apt-get update -y -y && sudo apt-get upgrade -y

            # Install Java (required by Elasticsearch)
            sudo apt-get install -y openjdk-11-jdk

            if ! systemctl list-units --type=service --all | grep -q elasticsearch.service; then
              echo "Installing Elasticsearch..."
              # Download and install the public signing key
              wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
              sudo apt-get install apt-transport-https
              echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
              sudo apt-get update -y && sudo apt-get install elasticsearch
              sudo /bin/systemctl daemon-reload
              sudo /bin/systemctl enable elasticsearch.service
              echo "waiting for elasticsearch to fully install before starting it"
              sleep 30 
              sudo systemctl start elasticsearch.service
              sleep 40
            else
              echo "Elasticsearch already installed."
              sudo systemctl restart elasticsearch
              sleep 10
            fi

            echo "Getting enrollment token..."
            TOKEN=$(sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana)
            echo "Exporting the token to use later"
            export ENROLLMENT_TOKEN="$TOKEN"

            # Install Kibana
            if ! systemctl list-units --type=service --all | grep -q kibana.service; then
              echo "Installing Kibana..."
              sudo apt-get update -y && sudo apt-get install kibana
              sudo systemctl enable kibana.service

              # Configure Kibana to listen on all interfaces
              sudo sed -i "s|^#\?server.host:.*|server.host: \"0.0.0.0\"|" /etc/kibana/kibana.yml

              # Add basePath configs and set elasticsearch address (if needed)
              echo 'server.basePath: "/elk"' | sudo tee -a /etc/kibana/kibana.yml
              echo 'server.rewriteBasePath: true' | sudo tee -a /etc/kibana/kibana.yml
              sudo sed -i "s|^#\?elasticsearch.hosts:.*|elasticsearch.hosts: [\"http://localhost:9200\"]|" /etc/kibana/kibana.yml

            else
              echo "Kibana already installed."
              sudo systemctl restart kibana
              sleep 10
            fi

            echo "passing token to kibana"
            sudo /usr/share/kibana/bin/kibana-setup --enrollment-token "$ENROLLMENT_TOKEN"
            echo "restarting kibana"
            sudo systemctl start kibana
            sleep 10
            sudo systemctl restart kibana
            
            # Install Logstash
            if ! systemctl list-units --type=service --all | grep -q logstash.service; then
              echo "Installing Logstash..."
              sudo apt-get update -y && sudo apt-get install logstash
              sudo systemctl enable logstash.service  
              sudo systemctl start logstash.service
            else
              echo "Logstash already installed."
              sudo systemctl restart logstash
              sleep 10
            fi

            echo "Configuring logstash"
              sudo tee /etc/logstash/conf.d/filebeat-to-es.conf > /dev/null << LOGSTASH_CONFIG
                input {
                  beats {
                    port => ${LOGSTASH_PORT}
                  }
                }

                filter {
                  # Optional: add filters here, like grok to parse logs
                }

                output {
                  elasticsearch {
                    hosts => ["http://localhost:${ELASTICSEARCH_PORT}"]
                    index => "filebeat-%{+YYYY.MM.dd}"
                  }
                }
          LOGSTASH_CONFIG
          echo "restarting logstash"
          sudo systemctl restart logstash

            # checks
            sudo systemctl status elasticsearch
            sudo systemctl status logstash
            sudo systemctl status kibana
          EOF

      # 4. Clean-up tunnel
      - name: Cleanup SSH Tunnel
        if: always() # Runs even if previous steps fail
        run: |
          # Kill tunnel process using port ${TUNNEL}
          pkill -f "ssh.*${TUNNEL}:.*${ELK}"
          echo "SSH tunnel terminated"

  terraform-destroy:
    name: "Terraform Destroy"
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    defaults:
      run:
        working-directory: infra/aws/
    # Only run if "destroy" is selected manually
    if: github.event.inputs.action == 'destroy'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.0.1

      - name: Terraform Init
        run: terraform init

      - name: Terraform Destroy
        run: terraform destroy -auto-approve
