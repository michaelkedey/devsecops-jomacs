name: Deploy ELK Stack
description: Deploy and configure the ELK stack

inputs:
  log_file:
    description: 'Name of the log file.'
    required: true
     
  EC2_USER:
    description: 'EC2 username for SSH connection.'
    required: true
     
  ELK_TUNNEL_PORT:
    description: 'Port used for SSH tunneling to the ELK instance.'
    required: true
     
  LOGSTASH_PORT:
    description: 'Port for Logstash input.'
    required: true
     
  ELASTICSEARCH_PORT:
    description: 'Port for Elasticsearch service.'
    required: true
     
  EC2_SSH_KEY:
    description: 'Private key for EC2 SSH access.'
    required: true
     
  AWS_ACCESS_KEY_ID:
    description: 'AWS Access Key ID for uploading logs.'
    required: true
     
  AWS_SECRET_ACCESS_KEY:
    description: 'AWS Secret Access Key for uploading logs.'
    required: true
     
  AWS_REGION:
    description: 'AWS region for S3 uploads.'
    required: true
     
  DEVSECOPS_GPG_PASSPHRASE:
    description: 'GPG passphrase used for encrypting files.'
    required: true


runs:
  using: "composite"
  steps:
    # 1. Checkout Code from GitHub
    - name: Checkout Code
      uses: actions/checkout@v4

    # 3. deploy elk
    - name: Deploy and start elk stack
      env:
        USER: ${{  inputs.EC2_USER }}
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
        LOGSTASH_PORT: ${{  inputs.LOGSTASH_PORT }}
        ELASTICSEARCH_PORT: ${{  inputs.ELASTICSEARCH_PORT }}
        PRIVATE_KEY: ${{ inputs.EC2_SSH_KEY }}
        GPG_PASSPHRASE: ${{  inputs.DEVSECOPS_GPG_PASSPHRASE }}
        LOGS_SEPARATOR: "=================================================================================================================================================================================="
      run: |
        set -e

        echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

        # SSH into the EC2 instance and install elk and start it
        ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no ${USER}@localhost << EOF | tee ${{ inputs.log_file }}         
          
          set -euo pipefail  # Exit on error, undefined variables, or pipe failures

          echo "${LOGS_SEPARATOR}"

          # Update package index
          echo "updating system"
          sudo apt-get update -y && sudo apt-get upgrade -y

          echo "Installing Java."
          # Install Java (required by Elasticsearch)
          echo "${LOGS_SEPARATOR}"
          sudo apt-get install -y openjdk-11-jdk

          echo "Installing Elasticsearch."
          echo "${LOGS_SEPARATOR}"
          if ! systemctl list-units --type=service --all | grep -q elasticsearch.service; then
            wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --batch --yes --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
            sudo apt-get install apt-transport-https
            echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/9.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-9.x.list
            sudo apt-get update && sudo apt-get install elasticsearch
            sleep 15
            sudo /bin/systemctl daemon-reload
            sudo /bin/systemctl enable elasticsearch.service
            sleep 30
            sudo systemctl start elasticsearch.service
            sleep 30
          else
            echo "Elasticsearch already installed."            
          fi

          echo "${LOGS_SEPARATOR}"
          echo "setting elsaticsearch configs"
          sudo sed -i '/^#\?cluster\.name:/d' /etc/elasticsearch/elasticsearch.yml && echo 'cluster.name: "elk"' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
          sudo sed -i '/^#\?network\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'network.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
          sudo sed -i '/^#\?transport\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'transport.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null

          echo "Generating the Elasticsearch password for the 'elastic' user."
          echo "${LOGS_SEPARATOR}"
          #sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -b | sed -n 's/^New value: //p' > elastic_password.txt
          ELASTIC_PASSWORD=\$(sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -b | sed -n 's/^New value: //p')
          sleep 15

          echo "${LOGS_SEPARATOR}"
          echo "gpg encryption of password"
          echo "\${ELASTIC_PASSWORD}" > elastic_password.txt
          gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --symmetric --cipher-algo AES256 elastic_password.txt
          rm elastic_password.txt

          echo "restarting elasticsearch"
          sudo systemctl restart elasticsearch


          echo "Installing Kibana."
          echo "${LOGS_SEPARATOR}"
          if ! systemctl list-units --type=service --all | grep -q kibana.service; then
            sudo apt-get update && sudo apt-get install kibana
            sleep 10
            sudo /bin/systemctl daemon-reload
            sudo /bin/systemctl enable kibana.service
            sleep 10
            sudo systemctl start kibana.service
          else
            echo "Kibana already installed."
          fi

          echo "${LOGS_SEPARATOR}"
          echo "setting kibana configs"
          sudo sed -i '/^#\?server\.host:/d' /etc/kibana/kibana.yml && \
          echo 'server.host: "0.0.0.0"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

          # Set server.basePath
          sudo sed -i '/^#\?server\.basePath:/d' /etc/kibana/kibana.yml && \
          echo 'server.basePath: "/elk"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

          # Set server.rewriteBasePath
          sudo sed -i '/^#\?server\.rewriteBasePath:/d' /etc/kibana/kibana.yml && \
          echo 'server.rewriteBasePath: true' | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          
          # Set elasticsearch.hosts
          sudo sed -i '/^#\?elasticsearch\.hosts:/d' /etc/kibana/kibana.yml && \
          echo 'elasticsearch.hosts: ["https://localhost:9200"]' | sudo tee -a /etc/kibana/kibana.yml > /dev/null          

          echo "${LOGS_SEPARATOR}"
          echo "setting kibana reporting key."
          # Generate a random 32-byte string, base64 encode it, and save to a file
          KIBANA_KEY=\$(sudo openssl rand -base64 32)
          
          sudo sed -i '/^#\?xpack\.reporting\.encryptionKey:/d' /etc/kibana/kibana.yml && \
          echo "xpack.reporting.encryptionKey: \"\${KIBANA_KEY}\"" | sudo tee -a /etc/kibana/kibana.yml > /dev/null

          sudo sed -i '/^#\?xpack\.encryptedSavedObjects\.encryptionKey:/d' /etc/kibana/kibana.yml && \
          echo "xpack.encryptedSavedObjects.encryptionKey: \"\${KIBANA_KEY}\"" | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          
          echo "configuring kibana certs" 
          sudo mkdir -p /etc/kibana/certs
          sudo cp /etc/elasticsearch/certs/http_ca.crt /etc/kibana/certs/
          sudo chown root:kibana /etc/kibana/certs/http_ca.crt
          sudo chmod 640 /etc/kibana/certs/http_ca.crt

          sudo sed -i '/^#\?elasticsearch\.ssl\.certificateAuthorities:/d' /etc/kibana/kibana.yml
          echo 'elasticsearch.ssl.certificateAuthorities: ["/etc/kibana/certs/http_ca.crt"]' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

          echo "${LOGS_SEPARATOR}"
          echo "setting enrolment token."
          TOKEN=\$(sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana)
          sleep 20
          
          echo "${LOGS_SEPARATOR}"
          echo "applying token to kibana setup."
          sudo /usr/share/kibana/bin/kibana-setup --enrollment-token "\$TOKEN"

          echo "restarting kibana"
          sudo systemctl restart kibana.service


          echo "${LOGS_SEPARATOR}"
          echo "Installing Logstash."
          if ! systemctl list-units --type=service --all | grep -q logstash.service; then
            echo "Installing Logstash..."
            sudo apt-get update -y && sudo apt-get install logstash
            sudo systemctl enable logstash.service  
            sudo systemctl start logstash.service
          else
            echo "Logstash already installed."
          fi

          echo "${LOGS_SEPARATOR}"
          echo "setting logstash configs"
          echo "Generate api key for logstash kibana auth"
          API_KEY_JSON=$(curl -s -X POST -u elastic:"$ELASTIC_PASSWORD" \
            -H "Content-Type: application/json" \
            "https://localhost:"${ELASTICSEARCH_PORT}"/_security/api_key?pretty" \
            -d '{
              "name": "logstash-api-key",
              "role_descriptors": {
                "logstash_writer": {
                  "cluster": ["monitor", "manage_index_templates"],
                  "indices": [
                    {
                      "names": ["flask-logs-*"],
                      "privileges": ["create_index", "write", "read"]
                    }
                  ]
                }
              }
            }')

          echo "${LOGS_SEPARATOR}"
          echo "Extracting API key credentials"
          API_KEY_ID=$(echo "$API_KEY_JSON" | jq -r '.id')
          API_KEY_SECRET=$(echo "$API_KEY_JSON" | jq -r '.api_key')

          echo "${LOGS_SEPARATOR}"
          echo "Creating Logstash keystore if not exists"
          sudo mkdir -p /etc/logstash
          sudo chown -R logstash:logstash /etc/logstash
          sudo chmod 755 /etc/logstash
          
          LOGSTASH_KEYSTORE_PASS=\$(openssl rand -base64 32)
          export LOGSTASH_KEYSTORE_PASS="\${LOGSTASH_KEYSTORE_PASS}"
          yes y | sudo -u logstash LOGSTASH_KEYSTORE_PASS="$LOGSTASH_KEYSTORE_PASS" \
          /usr/share/logstash/bin/logstash-keystore --path.settings /etc/logstash create

          echo "${LOGS_SEPARATOR}"
          echo "Storing key id and secret in Logstash keystore"
          echo "$API_KEY_ID" | sudo /usr/share/logstash/bin/logstash-keystore add ES_API_KEY_ID --stdin
          echo "$API_KEY_SECRET" | sudo /usr/share/logstash/bin/logstash-keystore add ES_API_KEY_SECRET --stdin
          
          echo "${LOGS_SEPARATOR}"
          echo "adding elasticsearch port to logstash keystore"
          echo "${ELASTICSEARCH_PORT}" | sudo /usr/share/logstash/bin/logstash-keystore add ELASTICSEARCH_PORT --stdin

          echo "${LOGS_SEPARATOR}"
          echo "logstash config file"
          sudo tee /etc/logstash/conf.d/filebeat-to-es.conf > /dev/null << LOGSTASH_CONFIG
            input {
              beats {
                port => ${LOGSTASH_PORT}
              }
            }

            filter {
              grok {
                match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level}: %{GREEDYDATA:msg}" }
              }
              date {
                match => ["timestamp", "ISO8601"]
              }
            }

            output {
              elasticsearch {
                hosts => ["https://localhost:%{ELASTICSEARCH_PORT}"]
                ssl => true
                ssl_certificate_verification => true
                cacert => "/etc/elasticsearch/certs/http_ca.crt"
                index => "flask-logs-%{+YYYY.MM.dd}"
                api_key => "${ES_API_KEY_ID}:${ES_API_KEY_SECRET}"
                retry_initial_interval => 3
                retry_max_interval => 60
                retry_on_conflict => 3
              }
            }

        LOGSTASH_CONFIG

          echo "restarting logstash"
          sudo systemctl restart logstash


          #checks
          sleep 20
          sudo systemctl status elasticsearch
          sudo systemctl status logstash
          sudo systemctl status kibana
        EOF
      shell: bash

    - name: Copy encrypted password from EC2 and upload
      env:
        USER: ${{  inputs.EC2_USER }}
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
        PRIVATE_KEY: ${{ inputs.EC2_SSH_KEY }}
        AWS_ACCESS_KEY_ID: ${{  inputs.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{  inputs.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{  inputs.AWS_REGION }}
      run: |
        echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

        echo "${LOGS_SEPARATOR}"
        echo "scp the gpg encrypted password"
        scp -i private_key.pem -P ${{ inputs.ELK_TUNNEL_PORT }} -o StrictHostKeyChecking=no ${USER}@localhost:/home/${USER}/elastic_password.txt.gpg .

        echo "upload password to s3"
        aws s3 cp elastic_password.txt.gpg s3://sedem-terra333-bucket/devsecops-jomacs/elastic_password.txt.gpg
      shell: bash

    - name: Encrypt and upload logs to s3
      env:
        AWS_ACCESS_KEY_ID: ${{  inputs.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{  inputs.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{  inputs.AWS_REGION }}
        GPG_PASSPHRASE: ${{  inputs.DEVSECOPS_GPG_PASSPHRASE }}
        LOGS_SEPARATOR: "=================================================================================================================================================================================="
      run: |
        
        echo "${LOGS_SEPARATOR}"
        echo "Encrypting and uploading the log file using the GPG passphrase"
        gpg --batch --yes --passphrase "${GPG_PASSPHRASE}" --symmetric --cipher-algo AES256 ${{ inputs.log_file }} 
        aws s3 cp ${{ inputs.log_file }}.gpg  s3://sedem-terra333-bucket/devsecops-jomacs/logs/${{ inputs.log_file }}.gpg

        echo "${LOGS_SEPARATOR}"
        echo "removing generated files"

        rm ${{ inputs.log_file }}.gpg
        rm ${{ inputs.log_file }}
        rm elastic_password.txt.gpg
      shell: bash

    # 4. Clean-up tunnel
    - name: Cleanup SSH Tunnel
      env:
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
      if: always() # Runs even if previous steps fail
      run: |
        # Kill tunnel process using port ${TUNNEL}
        pkill -f "ssh.*${TUNNEL}:.*${ELK}"
        rm -f private_key.pem
        echo "SSH tunnel terminated"
      shell: bash 
























# name: Deploy ELK Stack

# on:
#   workflow_call:  # Only callable by other workflows
#     inputs:
#       log_file:
#         required: true
#          

# jobs:
#   elk-deploy:
#       runs-on: ubuntu-latest
#       steps:
#         # 1. Checkout Code from GitHub
#         - name: Checkout Code
#           uses: actions/checkout@v4

#         # 3. deploy elk
#         - name: Deploy and start elk stack
#           env:
#             USER: ${{  inputs.EC2_USER }}
#             TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
#             LOGSTASH_PORT: ${{  inputs.LOGSTASH_PORT }}
#             ELASTICSEARCH_PORT: ${{  inputs.ELASTICSEARCH_PORT }}
#             PRIVATE_KEY: ${{  inputs.EC2_SSH_KEY }}
#              LOGS_SEPARATOR: "=================================================================================================================================================================================="
            
#           run: |
#             set -e
#             export LOGSTASH_PORT=${{  inputs.LOGSTASH_PORT }}
#             export ELASTICSEARCH_PORT=${{  inputs.ELASTICSEARCH_PORT }}
#             export LOGS_SEPARATOR=${LOGS_SEPARATOR}

#             echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

#             # SSH into the EC2 instance and install elk and start it
#             ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no ${USER}@localhost << 'EOF' | tee ${{ inputs.log_file }}         
#               echo "${LOGS_SEPARATOR}"

#               # Update package index
#               echo "updating system"
#               sudo apt-get update -y && sudo apt-get upgrade -y

#               echo "Installing Java."
#               # Install Java (required by Elasticsearch)
#               echo "${LOGS_SEPARATOR}"
#               sudo apt-get install -y openjdk-11-jdk

#               echo "Installing Elasticsearch."
#               echo "${LOGS_SEPARATOR}"
#               if ! systemctl list-units --type=service --all | grep -q elasticsearch.service; then
#                 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
#                 sudo apt-get install apt-transport-https
#                 echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/9.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-9.x.list
#                 sudo apt-get update && sudo apt-get install elasticsearch
#                 sleep 15
#                 sudo sed -i '/^#\?cluster\.name:/d' /etc/elasticsearch/elasticsearch.yml && echo 'cluster.name: "elk"' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
#                 sudo sed -i '/^#\?network\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'network.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
#                 sudo sed -i '/^#\?transport\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'transport.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
#                 sudo /bin/systemctl daemon-reload
#                 sudo /bin/systemctl enable elasticsearch.service
#                 sleep 30
#                 sudo systemctl start elasticsearch.service
#                 sleep 30
                  
#                 echo "Generating the Elasticsearch password for the 'elastic' user."
#                 echo "${LOGS_SEPARATOR}"
#                 ELASTIC_PASSWORD=$(sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -b | awk '/New value:/ {print $NF}')
#                 sleep 15
#                 echo "$ELASTIC_PASSWORD" > elastic_password.txt
                
#                 echo "FILE_TRANSFER_START"
#                 base64 elastic_password.txt
#                 echo "FILE_TRANSFER_END"

#                 rm elastic_password.txt
                  
#               else
#                 echo "Elasticsearch already installed."
#                 sudo systemctl restart elasticsearch
#                 sleep 10
#               fi

#               echo "Installing Kibana."
#               echo "${LOGS_SEPARATOR}"
#               if ! systemctl list-units --type=service --all | grep -q kibana.service; then
#                 sudo apt-get update && sudo apt-get install kibana
#                 sleep 10
#                 sudo sed -i '/^#\?server\.host:/d' /etc/kibana/kibana.yml && \
#                 echo 'server.host: "0.0.0.0"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

#                 # Set server.basePath
#                 sudo sed -i '/^#\?server\.basePath:/d' /etc/kibana/kibana.yml && \
#                 echo 'server.basePath: "/elk"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

#                 # Set server.rewriteBasePath
#                 sudo sed -i '/^#\?server\.rewriteBasePath:/d' /etc/kibana/kibana.yml && \
#                 echo 'server.rewriteBasePath: true' | sudo tee -a /etc/kibana/kibana.yml > /dev/null

#                 # Set elasticsearch.hosts
#                 sudo sed -i '/^#\?elasticsearch\.hosts:/d' /etc/kibana/kibana.yml && \
#                 echo 'elasticsearch.hosts: ["https://localhost:9200"]' | sudo tee -a /etc/kibana/kibana.yml > /dev/null
                
#                 echo "${LOGS_SEPARATOR}"
#                 echo "seting enrolment token."
#                 TOKEN=$(sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana)
#                 sleep 20

#                 echo "${LOGS_SEPARATOR}"
#                 echo "applying token to kibana setup."
#                 sudo /usr/share/kibana/bin/kibana-setup --enrollment-token "$TOKEN"

#                 sudo /bin/systemctl daemon-reload
#                 sudo /bin/systemctl enable kibana.service
#                 sleep 10
#                 sudo systemctl start kibana.service
#               else
#                 echo "Kibana already installed. Restarting..."
#                 sudo systemctl restart kibana
#               fi

#               echo "${LOGS_SEPARATOR}"
#               echo "Installing Logstash."
#               if ! systemctl list-units --type=service --all | grep -q logstash.service; then
#                 echo "Installing Logstash..."
#                 sudo apt-get update -y && sudo apt-get install logstash
#                 sudo systemctl enable logstash.service  
#                 sudo systemctl start logstash.service

#                 echo "${LOGS_SEPARATOR}"
#                 echo "Configuring logstash"
#                 sudo tee /etc/logstash/conf.d/filebeat-to-es.conf > /dev/null << LOGSTASH_CONFIG
#                   input {
#                     beats {
#                       port => ${LOGSTASH_PORT}
#                     }
#                   }

#                   filter {
#                     grok {
#                       match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level}: %{GREEDYDATA:msg}" }
#                     }
#                     date {
#                       match => ["timestamp", "ISO8601"]
#                     }
#                   }

#                   output {
#                     elasticsearch {
#                       hosts => ["https://localhost:${ELASTICSEARCH_PORT}"]
#                       index => "flask-logs-%{+YYYY.MM.dd}"
#                       user => "elastic"
#                       password => "${ELASTIC_PASSWORD}"
#                       ssl => true
#                       cacert => "/etc/elasticsearch/certs/http_ca.crt"
#                     }
#                   }
#             LOGSTASH_CONFIG
#               echo "restarting logstash"
#               sudo systemctl restart logstash

#               else
#                 echo "Logstash already installed."
#                 sudo systemctl restart logstash
#                 sleep 10
#               fi

#               checks
#               sudo systemctl status elasticsearch
#               sudo systemctl status logstash
#               sudo systemctl status kibana
#             EOF

#         - name: Encrypt and upload logs to s3
#           env:
#             AWS_ACCESS_KEY_ID: ${{  inputs.AWS_ACCESS_KEY_ID }}
#             AWS_SECRET_ACCESS_KEY: ${{  inputs.AWS_SECRET_ACCESS_KEY }}
#             AWS_REGION: ${{  inputs.AWS_REGION }}
#             GPG_PASSPHRASE: ${{  inputs.DEVSECOPS_GPG_PASSPHRASE }}
#              LOGS_SEPARATOR: "=================================================================================================================================================================================="
#           run: |
#             echo "${LOGS_SEPARATOR}"
#             echo "Extracting and encoding elasticsearch password from heredoc."
#             awk '/FILE_TRANSFER_START/{flag=1;next} /FILE_TRANSFER_END/{flag=0} flag' ${{ inputs.log_file }}  | base64 -d > elastic_password.txt
#             gpg --batch --yes --passphrase "${GPG_PASSPHRASE}" --symmetric --cipher-algo AES256 elastic_password.txt
#             aws s3 cp elastic_password.txt.gpg s3://sedem-terra333-bucket/devsecops-jomacs/elastic_password.txt.gpg

#             echo "${LOGS_SEPARATOR}"
#             echo "Encrypting and uploading the log file using the GPG passphrase stored in GitHub  inputs.==="
#             gpg --batch --yes --passphrase "${GPG_PASSPHRASE}" --symmetric --cipher-algo AES256 ${{ inputs.log_file }} 
#             aws s3 cp ${{ inputs.log_file }}.gpg  s3://sedem-terra333-bucket/devsecops-jomacs/logs/${{ inputs.log_file }}.gpg

#             echo "${LOGS_SEPARATOR}"
#             echo "removing generated files"
#             rm ${{ inputs.log_file }} 
#             rm ${{ inputs.log_file }}.gpg
#             rm elastic_password.txt
#             rm elastic_password.txt.gpg
            
#         # 4. Clean-up tunnel
#         - name: Cleanup SSH Tunnel
#           if: always() # Runs even if previous steps fail
#           run: |
#             # Kill tunnel process using port ${TUNNEL}
#             pkill -f "ssh.*${TUNNEL}:.*${ELK}"
#             rm -f private_key.pem
#             echo "SSH tunnel terminated"