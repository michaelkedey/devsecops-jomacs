name: Deploy ELK Stack
description: Deploy and configure the ELK stack

inputs:
  log_file:
    description: 'Name of the log file.'
    required: true
     
  EC2_USER:
    description: 'EC2 username for SSH connection.'
    required: true
     
  ELK_TUNNEL_PORT:
    description: 'Port used for SSH tunneling to the ELK instance.'
    required: true
     
  LOGSTASH_PORT:
    description: 'Port for Logstash input.'
    required: true
     
  ELASTICSEARCH_PORT:
    description: 'Port for Elasticsearch service.'
    required: true
     
  EC2_SSH_KEY:
    description: 'Private key for EC2 SSH access.'
    required: true
     
  AWS_ACCESS_KEY_ID:
    description: 'AWS Access Key ID for uploading logs.'
    required: true
     
  AWS_SECRET_ACCESS_KEY:
    description: 'AWS Secret Access Key for uploading logs.'
    required: true
     
  AWS_REGION:
    description: 'AWS region for S3 uploads.'
    required: true
     
  DEVSECOPS_GPG_PASSPHRASE:
    description: 'GPG passphrase used for encrypting files.'
    required: true

  ALB_DNS:
    description: 'loadbalancer url'
    required: true

  MAXIMIND_LICENSE_KEY:
    description: 'Maximind license key for geo-ip analytics'
    required: true


runs:
  using: "composite"
  steps:
    # 1. Checkout Code from GitHub
    - name: Checkout Code
      uses: actions/checkout@v4

    # 3. deploy elk
    - name: Deploy and start elk stack
      env:
        USER: ${{  inputs.EC2_USER }}
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
        LOGSTASH_PORT: ${{  inputs.LOGSTASH_PORT }}
        ELASTICSEARCH_PORT: ${{  inputs.ELASTICSEARCH_PORT }}
        PRIVATE_KEY: ${{ inputs.EC2_SSH_KEY }}
        GPG_PASSPHRASE: ${{  inputs.DEVSECOPS_GPG_PASSPHRASE }}
        ALB_DNS: ${{ inputs.ALB_DNS }}
        MAXIMIND_LICENSE_KEY: ${{ inputs.MAXIMIND_LICENSE_KEY }}
        LOGS_SEPARATOR: "=================================================================================================================================================================================="
      run: |
        set -e

        echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

        # SSH into the EC2 instance and install elk and start it
        ssh -i private_key.pem -p ${TUNNEL} -o StrictHostKeyChecking=no ${USER}@localhost << EOF | tee ${{ inputs.log_file }}         
          
          set -euo pipefail  # Exit on error, undefined variables, or pipe failures

          echo "${LOGS_SEPARATOR}"
          # Wait for apt lock to be released
          echo "Waiting for apt lock to start installing..."
          while sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1; do
            sleep 2
          done
          echo "apt lock ready"
          echo "updating system"
          sudo apt-get update -y && sudo apt-get upgrade -y

          echo "Installing Java."
          # Install Java (required by Elasticsearch)
          echo "${LOGS_SEPARATOR}"
          sudo apt-get install -y openjdk-11-jdk jq

          # echo "Installing Elasticsearch."
          # echo "${LOGS_SEPARATOR}"
          # if ! dpkg -s elasticsearch &>/dev/null; then
          #   wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --batch --yes --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
          #   sudo apt-get install apt-transport-https
          #   echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/9.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-9.x.list
          #   sudo apt-get update && sudo apt-get install elasticsearch
          #   sleep 15
          #   sudo /bin/systemctl daemon-reload
          #   sudo /bin/systemctl enable elasticsearch.service
          #   sleep 30
          #   echo "${LOGS_SEPARATOR}"
          #   echo "starting elasticsearch"
          #   sudo systemctl start elasticsearch.service
          #   sleep 30
          #   echo "${LOGS_SEPARATOR}"
          #   echo "regenerating elasticsearch password"
          #   ELASTIC_PASSWORD=\$(sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -b | sed -n 's/^New value: //p')
          #   sleep 10
          #   echo "${LOGS_SEPARATOR}"
          #   echo "gpg encryption of password"
          #   printf "%s\n" "\${ELASTIC_PASSWORD}" | gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --symmetric --cipher-algo AES256 -o elastic_password.txt.gpg
          #   echo "${LOGS_SEPARATOR}"
          #   echo "starting cluster"
          #   sudo systemctl start elasticsearch
          # else
          #   echo "Elasticsearch already installed."  
          # fi

          # echo "${LOGS_SEPARATOR}"
          # echo "setting elsaticsearch configs"
          # sudo sed -i '/^#\?cluster\.name:/d' /etc/elasticsearch/elasticsearch.yml && echo 'cluster.name: "elk"' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
          # sudo sed -i '/^#\?network\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'network.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
          # sudo sed -i '/^#\?transport\.host:/d' /etc/elasticsearch/elasticsearch.yml && echo 'transport.host: 0.0.0.0' | sudo tee -a /etc/elasticsearch/elasticsearch.yml > /dev/null
          # sudo cp /etc/elasticsearch/certs/http_ca.crt /usr/local/share/ca-certificates/http_ca.crt
          # sudo update-ca-certificates
          # echo "${LOGS_SEPARATOR}"
          # echo "restarting elasticsearch"
          # sudo systemctl restart elasticsearch.service
          # sleep 30


          echo "creating function to check cluster health"
          check_cluster_health() {
            local ELASTIC_PASSWORD=\$(gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --decrypt --quiet elastic_password.txt.gpg)
            local MAX_RETRIES=36
            local MAX_TOTAL_RETRIES=4
            local RETRY_COUNT=0
            local TOTAL_RETRIES=0

            echo "Checking Elasticsearch health..."
            until sudo curl -s -u elastic:"\$ELASTIC_PASSWORD" https://localhost:9200/_cluster/health \
              | grep -Eq '"status":"green"|"status":"yellow"'; do

              if [ "\$RETRY_COUNT" -ge "\$MAX_RETRIES" ]; then
                if [ "\$TOTAL_RETRIES" -ge "\$MAX_TOTAL_RETRIES" ]; then
                  echo " Maximum retry attempts reached. Exiting..."
                  return 1
                fi
                echo " Cluster unhealthy after \$((MAX_RETRIES * 5)) seconds. Restarting Elasticsearch..."
                sudo systemctl restart elasticsearch
                sleep 60
                RETRY_COUNT=0
                MAX_RETRIES=18
                TOTAL_RETRIES=\$((TOTAL_RETRIES + 1))
              else
                echo "Waiting for cluster to become healthy... (\$RETRY_COUNT/\$MAX_RETRIES)"
                sleep 5
                RETRY_COUNT=\$((RETRY_COUNT + 1))
              fi
            done
            echo " Elasticsearch cluster is healthy."
          }


          # echo "Installing Kibana."
          # echo "${LOGS_SEPARATOR}"
          # if ! dpkg -s kibana &>/dev/null; then
          #   sudo apt-get update && sudo apt-get install kibana
          #   sleep 10
          #   sudo /bin/systemctl daemon-reload
          #   sudo /bin/systemctl enable kibana.service
          #   sleep 10
          #   sudo systemctl start kibana.service

          #   echo "${LOGS_SEPARATOR}"
          #   echo "checking cluster health to set kibana token"
          #   check_cluster_health

          #   echo "${LOGS_SEPARATOR}"
          #   echo "setting enrolment token."
          #   TOKEN=\$(sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana)
          #   sleep 20
          #   echo "applying token to kibana setup."
          #   sudo /usr/share/kibana/bin/kibana-setup --enrollment-token "\$TOKEN"
          #   echo "${LOGS_SEPARATOR}"
          #   echo "setting kibana reporting key."
          #   # Generate a random 32-byte string, base64 encode it
          #   KIBANA_KEY=\$(sudo openssl rand -base64 32)
          #   echo "settign kibana reporting key"
          #   sudo sed -i '/^#\?xpack\.reporting\.encryptionKey:/d' /etc/kibana/kibana.yml && echo "xpack.reporting.encryptionKey: \"\${KIBANA_KEY}\"" | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          #   sudo sed -i '/^#\?xpack\.encryptedSavedObjects\.encryptionKey:/d' /etc/kibana/kibana.yml && echo "xpack.encryptedSavedObjects.encryptionKey: \"\${KIBANA_KEY}\"" | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          #   echo "${LOGS_SEPARATOR}"
          #   echo "gpg encryption of token and key"
          #   echo "\${TOKEN}" > kibana_setup_token.txt
          #   echo "\${KIBANA_KEY}" > kibana_reporting_key.txt
          #   printf "%s\n" "\${TOKEN}" | gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --symmetric --cipher-algo AES256 -o kibana_setup_token.gpg
          #   printf "%s\n" "\${KIBANA_KEY}" | gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --symmetric --cipher-algo AES256 -o kibana_reporting_key.gpg
          # else
          #   echo "Kibana already installed."
          # fi

          # echo "${LOGS_SEPARATOR}"
          # echo "setting kibana configs"
          # echo "configuring kibana certs" 
          # sudo mkdir -p /etc/kibana/certs
          # sudo cp /etc/elasticsearch/certs/http_ca.crt /etc/kibana/certs/
          # sudo chown root:kibana /etc/kibana/certs/http_ca.crt
          # sudo chmod 640 /etc/kibana/certs/http_ca.crt
          # sudo sed -i '/^#\?server\.host:/d' /etc/kibana/kibana.yml && echo 'server.host: "0.0.0.0"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          # sudo sed -i '/^#\?server\.basePath:/d' /etc/kibana/kibana.yml && echo 'server.basePath: "/elk"' | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          # sudo sed -i '/^server\.publicBaseUrl:/d' /etc/kibana/kibana.yml && echo "server.publicBaseUrl: \"http://${ALB_DNS}/elk\"" | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          # sudo sed -i '/^#\?server\.rewriteBasePath:/d' /etc/kibana/kibana.yml && echo 'server.rewriteBasePath: true' | sudo tee -a /etc/kibana/kibana.yml > /dev/null
          # sudo sed -i '/^#\?elasticsearch\.hosts:/d' /etc/kibana/kibana.yml && echo 'elasticsearch.hosts: ["https://localhost:9200"]' | sudo tee -a /etc/kibana/kibana.yml > /dev/null          
          # sudo sed -i '/^#\?elasticsearch\.ssl\.certificateAuthorities:/d' /etc/kibana/kibana.yml && echo 'elasticsearch.ssl.certificateAuthorities: ["/etc/kibana/certs/http_ca.crt"]' | sudo tee -a /etc/kibana/kibana.yml > /dev/null  
          # echo "${LOGS_SEPARATOR}"
          # echo "restarting kibana"
          # sudo systemctl restart kibana.service


          # echo "${LOGS_SEPARATOR}"
          # echo "Installing Logstash."
          # if ! dpkg -s logstash &>/dev/null; then
          #   echo "Installing Logstash..."
          #   sudo apt-get update -y && sudo apt-get install logstash
          #   echo "${LOGS_SEPARATOR}"
          #   echo "Maximind geolocation db for logstash"
          #   wget "https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-City&license_key=$MAXIMIND_LICENSE_KEY&suffix=tar.gz" -O GeoLite2-City.tar.gz
          #   tar -xvzf GeoLite2-City.tar.gz
          #   DB_DIR=\$(find . -type d -name 'GeoLite2-City_*' | head -n 1)
          #   sudo mkdir -p /etc/logstash/geoip
          #   cp "$DB_DIR/GeoLite2-City.mmdb" /etc/logstash/geoip/GeoLite2-City.mmdb
          #   echo "${LOGS_SEPARATOR}"
          #   echo "starting logstash"
          #   sudo systemctl enable logstash.service  
          #   sudo systemctl start logstash.service
            
          #   # echo "${LOGS_SEPARATOR}"
          #   # echo "Creating Logstash keystore if not exists"
          #   # sudo mkdir -p /etc/logstash
          #   # sudo chown -R logstash:logstash /etc/logstash
          #   # sudo chmod 755 /etc/logstash
            
          #   # #LOGSTASH_KEYSTORE_PASS=\$(openssl rand -base64 32)
          #   # LOGSTASH_KEYSTORE_PASS=$(openssl rand -base64 16 | tr -dc '[:alnum:]')  # Ensure ASCII characters only
          #   # export LOGSTASH_KEYSTORE_PASS
          #   # echo y | sudo --preserve-env=LOGSTASH_KEYSTORE_PASS -u logstash \
          #   # /usr/share/logstash/bin/logstash-keystore --path.settings /etc/logstash create
          # else
          #   echo "Logstash already installed."
          # fi

          echo "${LOGS_SEPARATOR}"
          wget "https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-City&license_key=$MAXIMIND_LICENSE_KEY&suffix=tar.gz" -O GeoLite2-City.tar.gz
          tar -xvzf GeoLite2-City.tar.gz
          DB_DIR=\$(find . -type d -name 'GeoLite2-City_*' | head -n 1)
          sudo mkdir -p /etc/logstash/geoip
          sudo cp "\$DB_DIR/GeoLite2-City.mmdb" /etc/logstash/geoip/GeoLite2-City.mmdb
          echo "${LOGS_SEPARATOR}"
          echo "starting logstash"
          sudo systemctl enable logstash.service  
          sudo systemctl start logstash.service

          echo "${LOGS_SEPARATOR}"
          echo "checking cluster health to set logstash api key"
          check_cluster_health
          
          echo "${LOGS_SEPARATOR}"
          echo "Generate api key for logstash kibana auth"
          export ELASTIC_PASSWORD=\$(gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --decrypt --quiet elastic_password.txt.gpg)
          LOGSTASH_API_KEY=\$(curl -s -X POST -u "elastic:\${ELASTIC_PASSWORD}" \
            -H "Content-Type: application/json" \
            "https://localhost:9200/_security/api_key?pretty" \
            -d '{
              "name": "logstash-api-key",
              "role_descriptors": {
                "logstash_writer": {
                  "cluster": ["monitor", "manage_index_templates"],
                  "index": [{
                    "names": ["flask-logs-*"],
                    "privileges": ["create_index", "write", "read"]
                  }]
                }
              }
            }' | jq -r '"\(.id):\(.api_key)"')

          echo "${LOGS_SEPARATOR}"
          echo "GPg encryption of API keys"
          # Write raw JSON output with proper escaping
          printf "%s\n" "\${LOGSTASH_API_KEY}" > logstash_api_key
          gpg --batch --yes --passphrase "$GPG_PASSPHRASE" --symmetric --cipher-algo AES256 logstash_api_key
          echo "${LOGS_SEPARATOR}"

          echo "${LOGS_SEPARATOR}"
          echo "logstash config file"
          echo "configuring logstash certs" 
          sudo mkdir -p /etc/logstash/certs
          sudo cp /etc/elasticsearch/certs/http_ca.crt /etc/logstash/certs/
          sudo chown root:logstash /etc/logstash/certs/http_ca.crt
          sudo chmod 640 /etc/logstash/certs/http_ca.crt
          sudo /usr/share/logstash/bin/logstash-plugin install logstash-filter-geoip
          sudo tee /etc/logstash/conf.d/filebeat-to-es.conf > /dev/null << LOGSTASH
            input {
              beats {
                port => ${LOGSTASH_PORT}
                host => "0.0.0.0"
              }
            }

            filter {
              # Parse the entire log line as JSON
              json {
                source => "message"
                target => "parsed_json"
              }

              # Promote fields from parsed_json to root level
              mutate {
                rename => {
                  "[parsed_json][asctime]"          => "asctime"
                  "[parsed_json][levelname]"        => "levelname"
                  "[parsed_json][message]"          => "flask_message"
                  "[parsed_json][pathname]"         => "pathname"
                  "[parsed_json][lineno]"           => "lineno"
                  "[parsed_json][event]"            => "flask_event"
                  "[parsed_json][ip]"               => "ip"
                  "[parsed_json][resolved_city]"    => "resolved_city"
                  "[parsed_json][resolved_country]" => "resolved_country"
                  "[parsed_json][user_agent]"       => "user_agent"
                  "[parsed_json][path]"             => "flask_path"
                }
                remove_field => ["parsed_json", "message"]
              }

              # Add geolocation data based on the IP
              geoip {
                source => "ip"
                target => "geoip"
                database => "/etc/logstash/geoip/GeoLite2-City.mmdb"
                add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
                add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
              }
              mutate {
                convert => [ "[geoip][coordinates]", "float"]
              }

              # Parse the timestamp
              date {
                match => ["asctime", "YYYY-MM-dd HH:mm:ss,SSS"]
                target => "@timestamp"
              }
            }

            output {
              elasticsearch {
                hosts => ["https://localhost:${ELASTICSEARCH_PORT}"]
                ssl_enabled => true
                ssl_verification_mode => full
                ssl_certificate_authorities => "/etc/logstash/certs/http_ca.crt"
                index => "flask-logs-%{+YYYY.MM.dd}"
                api_key => "\${LOGSTASH_API_KEY}"
                retry_initial_interval => 3
                retry_max_interval => 60
                retry_on_conflict => 3
              }
            }

        LOGSTASH
          echo "restarting logstash"
          sudo systemctl restart logstash
          sleep 60

          #check_logstash_health()
          check_logstash_health() {
            MAX_RETRIES=4
            ATTEMPT=1
            echo "${LOGS_SEPARATOR}"
            echo "checking logstash health"
            # Check for recent Logstash failures
            if sudo journalctl -u logstash --since "2 minutes ago" --no-pager | grep -Ei "failed|error|fatal|pipeline aborted|bootstrap checks failed"; then
              echo "Logstash failure detected. Starting recovery attempts..."
              while [ "\$ATTEMPT" -le "\$MAX_RETRIES" ]; do
                echo "Attempt \$ATTEMPT to restart Logstash..."
                if sudo systemctl restart logstash; then
                  sleep 10  # Give it time to start
                  # Check if Logstash is now active
                  if systemctl is-active --quiet logstash; then
                    echo "Logstash restarted successfully on attempt \$ATTEMPT."
                    return 0
                  else
                    echo "Logstash still inactive after restart."
                  fi
                else
                  echo "systemctl restart failed."
                fi
                ATTEMPT=\$((ATTEMPT + 1))
                sleep 10
              done

              # If all attempts fail, log the failure
              echo "All \$MAX_RETRIES attempts failed. Logstash remains inactive."
              return 1
            else
              # No recent failures detected
              echo "No recent Logstash failures found."
            fi
            echo "${LOGS_SEPARATOR}"
          }

          
          #checks
          echo "${LOGS_SEPARATOR}"
          echo "checking cluster health to check status"
          check_cluster_health
          check_logstash_health

          echo "${LOGS_SEPARATOR}"
          echo "service status checks"
          sudo systemctl status elasticsearch
          sudo systemctl status logstash
          sudo systemctl status kibana
        EOF
      shell: bash

    - name: Copy encrypted password from EC2 and upload
      env:
        USER: ${{  inputs.EC2_USER }}
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
        PRIVATE_KEY: ${{ inputs.EC2_SSH_KEY }}
        AWS_ACCESS_KEY_ID: ${{  inputs.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{  inputs.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{  inputs.AWS_REGION }}
      run: |
        echo "$PRIVATE_KEY" > private_key.pem && chmod 600 private_key.pem

        echo "${LOGS_SEPARATOR}"
        echo "scp the gpg encrypted password"
        scp -i private_key.pem -P ${{ inputs.ELK_TUNNEL_PORT }} -o StrictHostKeyChecking=no ${USER}@localhost:/home/${USER}/elastic_password.txt.gpg .

        echo "upload password to s3"
        aws s3 cp elastic_password.txt.gpg s3://sedem-terra333-bucket/devsecops-jomacs/elastic_password.txt.gpg
      shell: bash

    - name: Encrypt and upload logs to s3
      env:
        AWS_ACCESS_KEY_ID: ${{  inputs.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{  inputs.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{  inputs.AWS_REGION }}
        GPG_PASSPHRASE: ${{  inputs.DEVSECOPS_GPG_PASSPHRASE }}
        LOGS_SEPARATOR: "=================================================================================================================================================================================="
      run: |
        
        echo "${LOGS_SEPARATOR}"
        echo "Encrypting and uploading the log file using the GPG passphrase"
        gpg --batch --yes --passphrase "${GPG_PASSPHRASE}" --symmetric --cipher-algo AES256 ${{ inputs.log_file }} 
        aws s3 cp ${{ inputs.log_file }}.gpg  s3://sedem-terra333-bucket/devsecops-jomacs/logs/${{ inputs.log_file }}.gpg

        echo "${LOGS_SEPARATOR}"
        echo "removing generated files"

        rm ${{ inputs.log_file }}.gpg
        rm ${{ inputs.log_file }}
        rm elastic_password.txt.gpg
      shell: bash

    # 4. Clean-up tunnel
    - name: Cleanup SSH Tunnel
      env:
        TUNNEL: ${{  inputs.ELK_TUNNEL_PORT }}
      if: always() # Runs even if previous steps fail
      run: |
        # Kill tunnel process using port ${TUNNEL}
        pkill -f "ssh.*${TUNNEL}:.*${ELK}"
        rm -f private_key.pem
        echo "SSH tunnel terminated"
      shell: bash 
